{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04469952",
      "metadata": {
        "id": "04469952"
      },
      "source": [
        "# PyTorch from the Ground Up\n",
        "\n",
        "A complete, practical guide -- from tensors to training loops.\n",
        "\n",
        "**Contents:**\n",
        "1. What is PyTorch?\n",
        "2. Tensors -- The Foundation\n",
        "3. Tensor Operations\n",
        "4. Autograd -- How PyTorch Learns\n",
        "5. Building a Neural Network (Manual Way)\n",
        "6. `torch.nn` -- The High-Level API\n",
        "7. The Standard Training Loop\n",
        "8. Data -- Dataset and DataLoader\n",
        "9. GPU Usage\n",
        "10. Saving & Loading Models\n",
        "11. The Batch Dimension\n",
        "12. The Full Mental Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c23b37e",
      "metadata": {
        "id": "0c23b37e"
      },
      "source": [
        "## Setup\n",
        "PyTorch comes pre-installed on Google Colab. If running locally:\n",
        "```bash\n",
        "pip install torch torchvision\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6effb4bf",
      "metadata": {
        "id": "6effb4bf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS available (Apple Silicon): {torch.backends.mps.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f86726f",
      "metadata": {
        "id": "8f86726f"
      },
      "source": [
        "---\n",
        "## 1. What is PyTorch?\n",
        "\n",
        "PyTorch is a deep learning framework built on two core ideas:\n",
        "\n",
        "- **Tensors** -- n-dimensional arrays (like NumPy) that can run on GPU\n",
        "- **Autograd** -- automatic differentiation engine that computes gradients for you\n",
        "\n",
        "Everything else (neural networks, optimizers, etc.) is built on top of these.\n",
        "\n",
        "### Why not just use NumPy?\n",
        "\n",
        "NumPy arrays live only on **CPU**. PyTorch tensors can live on **GPU**, which has thousands of cores doing parallel math. A matrix multiply that takes 500ms on CPU might take 2ms on GPU -- the difference between training in weeks vs hours.\n",
        "\n",
        "**Key insight:** Tensors are not just data containers -- they are the *computation substrate*. Everything flows through them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82cd262",
      "metadata": {
        "id": "f82cd262"
      },
      "source": [
        "---\n",
        "## 2. Tensors -- The Foundation\n",
        "\n",
        "A tensor is just a number, vector, matrix, or any n-dimensional array.\n",
        "\n",
        "**Why tensors over plain lists?**\n",
        "- GPU support -- run on thousands of parallel cores\n",
        "- Built-in math operations (matmul, sum, etc.)\n",
        "- Autograd integration -- gradients flow through them\n",
        "- Efficient memory layout (contiguous C arrays under the hood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd25207",
      "metadata": {
        "id": "bbd25207"
      },
      "outputs": [],
      "source": [
        "# Scalar (0D tensor)\n",
        "a = torch.tensor(3.14)\n",
        "print(f\"Scalar: {a}\")\n",
        "print(f\"Shape: {a.shape}\")     # torch.Size([])\n",
        "print(f\"Dimensions: {a.ndim}\") # 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b0dcff",
      "metadata": {
        "id": "67b0dcff"
      },
      "outputs": [],
      "source": [
        "# Vector (1D)\n",
        "b = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(f\"Vector: {b}\")\n",
        "print(f\"Shape: {b.shape}\")  # torch.Size([3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f65e7df",
      "metadata": {
        "id": "0f65e7df"
      },
      "outputs": [],
      "source": [
        "# Matrix (2D)\n",
        "c = torch.tensor([[1, 2], [3, 4]])\n",
        "print(f\"Matrix:\\n{c}\")\n",
        "print(f\"Shape: {c.shape}\")  # torch.Size([2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d75eeb",
      "metadata": {
        "id": "c0d75eeb"
      },
      "outputs": [],
      "source": [
        "# 3D tensor -- e.g., batch of grayscale images: [batch, height, width]\n",
        "d = torch.zeros(8, 28, 28)\n",
        "print(f\"3D tensor shape: {d.shape}\")  # torch.Size([8, 28, 28])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b2e6d3",
      "metadata": {
        "id": "92b2e6d3"
      },
      "source": [
        "### Creating Tensors -- Common Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c29378f",
      "metadata": {
        "id": "7c29378f"
      },
      "outputs": [],
      "source": [
        "print(\"zeros:    \", torch.zeros(3, 4))\n",
        "print(\"ones:     \", torch.ones(2, 3))\n",
        "print(\"rand:     \", torch.rand(2, 3))        # uniform [0, 1)\n",
        "print(\"randn:    \", torch.randn(2, 3))       # standard normal\n",
        "print(\"arange:   \", torch.arange(0, 10, 2)) # [0, 2, 4, 6, 8]\n",
        "print(\"linspace: \", torch.linspace(0, 1, 5))\n",
        "print(\"eye:    \\n\", torch.eye(3))           # identity matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd33a24",
      "metadata": {
        "id": "0dd33a24"
      },
      "source": [
        "### Data Types (dtypes)\n",
        "\n",
        "| dtype | Use case |\n",
        "|-------|----------|\n",
        "| `float32` | Default for ML -- best balance of speed and precision |\n",
        "| `float16` / `bfloat16` | Large model training -- saves memory |\n",
        "| `int64` | Class labels, indices |\n",
        "| `bool` | Masks |\n",
        "\n",
        "**Why dtype matters:** float16 uses half the memory of float32. Wrong dtype can cause silent incorrect results or runtime errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950f55b2",
      "metadata": {
        "id": "950f55b2"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1.0, 2.0])    # float32 by default\n",
        "print(f\"dtype: {x.dtype}\")\n",
        "\n",
        "y = torch.tensor([1, 2])        # int64 by default\n",
        "print(f\"dtype: {y.dtype}\")\n",
        "\n",
        "# Casting\n",
        "print(\"to float64:\", x.to(torch.float64))\n",
        "print(\"to int:    \", x.int())\n",
        "print(\"to bool:   \", x.bool())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67bab41d",
      "metadata": {
        "id": "67bab41d"
      },
      "source": [
        "---\n",
        "## 3. Tensor Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9cb2bb",
      "metadata": {
        "id": "6c9cb2bb"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "print(f\"a + b  = {a + b}\")\n",
        "print(f\"a * b  = {a * b}\")\n",
        "print(f\"a ** 2 = {a ** 2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0afc8780",
      "metadata": {
        "id": "0afc8780"
      },
      "outputs": [],
      "source": [
        "print(f\"sum:  {a.sum()}\")\n",
        "print(f\"mean: {a.mean()}\")\n",
        "print(f\"max:  {a.max()}\")\n",
        "print(f\"std:  {a.std()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41833156",
      "metadata": {
        "id": "41833156"
      },
      "outputs": [],
      "source": [
        "# Matrix multiplication -- the most important operation in deep learning\n",
        "# Every linear layer is just a matmul\n",
        "A = torch.randn(3, 4)\n",
        "B = torch.randn(4, 5)\n",
        "C = A @ B           # same as torch.matmul(A, B)\n",
        "print(f\"A: {A.shape}, B: {B.shape}, A@B: {C.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4c19d8",
      "metadata": {
        "id": "da4c19d8"
      },
      "outputs": [],
      "source": [
        "# Reshaping -- you will use this constantly\n",
        "x = torch.arange(12, dtype=torch.float32)\n",
        "print(f\"Original: {x.shape}\")\n",
        "print(f\"reshape(3, 4):  {x.reshape(3, 4).shape}\")\n",
        "print(f\"reshape(3, -1): {x.reshape(3, -1).shape}\")   # -1 = infer\n",
        "print(f\"unsqueeze(0):   {x.unsqueeze(0).shape}\")      # [12] -> [1, 12]\n",
        "print(f\"unsqueeze(1):   {x.unsqueeze(1).shape}\")      # [12] -> [12, 1]\n",
        "y = torch.zeros(1, 12)\n",
        "print(f\"squeeze:        {y.squeeze().shape}\")          # [1, 12] -> [12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c55398",
      "metadata": {
        "id": "08c55398"
      },
      "outputs": [],
      "source": [
        "# Indexing & slicing -- same as NumPy\n",
        "x = torch.randn(4, 3)\n",
        "print(f\"x shape: {x.shape}\")\n",
        "print(f\"First row:      {x[0]}\")\n",
        "print(f\"Second column:  {x[:, 1]}\")\n",
        "print(f\"Rows 1-2 shape: {x[1:3, :].shape}\")\n",
        "print(f\"Boolean mask:   {x[x > 0][:4]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0aa527",
      "metadata": {
        "id": "ff0aa527"
      },
      "source": [
        "---\n",
        "## 4. Autograd -- How PyTorch Learns\n",
        "\n",
        "### Why Automatic Differentiation?\n",
        "\n",
        "Training requires computing gradients -- how much does each parameter contribute to the error?\n",
        "\n",
        "For a network with millions of parameters, doing this by hand is impossible. Before autograd, researchers had to **manually derive and code gradient equations** for every architecture.\n",
        "\n",
        "**What autograd does:** You write the forward computation. PyTorch records every operation as a computational graph. `backward()` walks the graph backwards and applies the chain rule automatically.\n",
        "\n",
        "PyTorch is **define-by-run** -- the graph is built dynamically as you execute. You can use normal Python debugging, conditionals, loops -- no restrictions.\n",
        "\n",
        "When you mark a tensor with `requires_grad=True`, PyTorch tracks every operation on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3686ea6b",
      "metadata": {
        "id": "3686ea6b"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# y = x^2 + 2x + 1\n",
        "y = x ** 2 + 2*x + 1\n",
        "\n",
        "y.backward()   # compute dy/dx\n",
        "\n",
        "# dy/dx = 2x + 2 = 2(3) + 2 = 8\n",
        "print(f\"x = {x.item()}\")\n",
        "print(f\"y = {y.item()}\")\n",
        "print(f\"dy/dx = {x.grad.item()}\")   # 8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee011583",
      "metadata": {
        "id": "ee011583"
      },
      "outputs": [],
      "source": [
        "# Multiple variables -- chain rule applied automatically\n",
        "a = torch.tensor(2.0, requires_grad=True)\n",
        "b = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "c = a * b        # c = ab\n",
        "d = c + a        # d = ab + a\n",
        "\n",
        "d.backward()\n",
        "\n",
        "# d(d)/d(a) = b + 1 = 4\n",
        "# d(d)/d(b) = a = 2\n",
        "print(f\"d(d)/d(a) = {a.grad.item()}\")   # 4.0\n",
        "print(f\"d(d)/d(b) = {b.grad.item()}\")   # 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9c8112a",
      "metadata": {
        "id": "e9c8112a"
      },
      "outputs": [],
      "source": [
        "# torch.no_grad() -- stop tracking (for inference)\n",
        "# Benefit: saves ~50% memory + faster\n",
        "# When to use: ALWAYS during validation and inference\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "print(f\"requires_grad: {y.requires_grad}\")           # True\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = x * 2\n",
        "    print(f\"requires_grad (no_grad): {y.requires_grad}\")  # False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d995bb4",
      "metadata": {
        "id": "3d995bb4"
      },
      "source": [
        "### Why `zero_grad()` is Critical\n",
        "\n",
        "PyTorch **accumulates** gradients by default. Each `backward()` *adds* to existing gradients rather than replacing them.\n",
        "\n",
        "Forgetting `zero_grad()` is one of the most common PyTorch bugs -- the model still runs, just trains incorrectly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206c0b09",
      "metadata": {
        "id": "206c0b09"
      },
      "outputs": [],
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "print(\"--- Without zero_grad (WRONG) ---\")\n",
        "for i in range(3):\n",
        "    loss = w * 2\n",
        "    loss.backward()\n",
        "    print(f\"  Step {i+1} grad: {w.grad.item()}\")  # 2, 4, 6 -- accumulates!\n",
        "\n",
        "print()\n",
        "\n",
        "w2 = torch.tensor(1.0, requires_grad=True)\n",
        "print(\"--- With zero_grad (CORRECT) ---\")\n",
        "for i in range(3):\n",
        "    loss = w2 * 2\n",
        "    loss.backward()\n",
        "    print(f\"  Step {i+1} grad: {w2.grad.item()}\")  # 2, 2, 2 -- correct!\n",
        "    w2.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb785744",
      "metadata": {
        "id": "eb785744"
      },
      "source": [
        "---\n",
        "## 5. Building a Neural Network -- The Manual Way\n",
        "\n",
        "Before using high-level APIs, let us see what happens underneath.\n",
        "\n",
        "**Goal:** Learn `y = 3x + 2` from noisy data using manual gradient descent.\n",
        "\n",
        "**The training cycle:**\n",
        "1. Forward pass -- compute predictions\n",
        "2. Compute loss -- measure error\n",
        "3. Backward pass -- compute gradients\n",
        "4. Update parameters -- move to reduce loss\n",
        "5. Zero gradients -- reset for next step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2ebd62",
      "metadata": {
        "id": "3b2ebd62"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Generate fake data: y = 3x + 2 + noise\n",
        "X = torch.randn(100, 1)\n",
        "y = 3 * X + 2 + 0.1 * torch.randn(100, 1)\n",
        "\n",
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52345f72",
      "metadata": {
        "id": "52345f72"
      },
      "outputs": [],
      "source": [
        "# Parameters to learn\n",
        "W = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.zeros(1,    requires_grad=True)\n",
        "\n",
        "print(f\"Initial W: {W.item():.4f}  (target: 3.0)\")\n",
        "print(f\"Initial b: {b.item():.4f}  (target: 2.0)\")\n",
        "\n",
        "lr = 0.1\n",
        "losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    # 1. Forward\n",
        "    y_pred = X @ W + b\n",
        "\n",
        "    # 2. Loss (MSE)\n",
        "    loss = ((y_pred - y) ** 2).mean()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # 3. Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 4. Update\n",
        "    with torch.no_grad():\n",
        "        W -= lr * W.grad\n",
        "        b -= lr * b.grad\n",
        "\n",
        "    # 5. Zero gradients\n",
        "    W.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "print(f\"\\nLearned W: {W.item():.4f}  (true: 3.0)\")\n",
        "print(f\"Learned b: {b.item():.4f}  (true: 2.0)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19e99f5",
      "metadata": {
        "id": "e19e99f5"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(losses)\n",
        "ax1.set_title('Training Loss'); ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('MSE'); ax1.set_yscale('log'); ax1.grid(True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_plot = torch.linspace(-3, 3, 100).unsqueeze(1)\n",
        "    y_plot = x_plot @ W + b\n",
        "\n",
        "ax2.scatter(X.numpy(), y.numpy(), alpha=0.4, label='Data')\n",
        "ax2.plot(x_plot.numpy(), y_plot.numpy(), 'r-', linewidth=2,\n",
        "         label=f'y={W.item():.2f}x+{b.item():.2f}')\n",
        "ax2.set_title('Result'); ax2.legend(); ax2.grid(True)\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d267521",
      "metadata": {
        "id": "2d267521"
      },
      "source": [
        "---\n",
        "## 6. `torch.nn` -- The High-Level API\n",
        "\n",
        "### Why `nn.Module`?\n",
        "\n",
        "As networks grow, you need:\n",
        "- Automatic parameter tracking (for the optimizer)\n",
        "- Train/eval mode switching (dropout, batchnorm behave differently)\n",
        "- Clean save/load\n",
        "- Modular composition of sub-networks\n",
        "\n",
        "`nn.Module` provides all of this. `model.parameters()` recursively finds every parameter. `model.to(device)` moves everything. `model.train()` / `model.eval()` sets mode for every layer.\n",
        "\n",
        "**Always call `model(x)`, NOT `model.forward(x)` directly.**\n",
        "The `__call__` method wraps `forward()` with hooks and other important machinery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "063d7f34",
      "metadata": {
        "id": "063d7f34"
      },
      "outputs": [],
      "source": [
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)   # in_features=1, out_features=1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LinearModel()\n",
        "print(model)\n",
        "print()\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"  {name}: shape={param.shape}, value={param.data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8145e523",
      "metadata": {
        "id": "8145e523"
      },
      "outputs": [],
      "source": [
        "# Same linear regression using nn -- much cleaner\n",
        "model = LinearModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "    y_pred = model(X)               # forward\n",
        "    loss = loss_fn(y_pred, y)       # loss\n",
        "\n",
        "    optimizer.zero_grad()           # zero BEFORE backward\n",
        "    loss.backward()                 # gradients\n",
        "    optimizer.step()                # update\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "print(f\"W: {model.linear.weight.item():.4f}  (true: 3.0)\")\n",
        "print(f\"b: {model.linear.bias.item():.4f}  (true: 2.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e5bff8",
      "metadata": {
        "id": "b4e5bff8"
      },
      "source": [
        "### Common Layers\n",
        "\n",
        "| Layer | Purpose | When to Use |\n",
        "|-------|---------|-------------|\n",
        "| `nn.Linear(in, out)` | Fully connected | Tabular data, classifier heads |\n",
        "| `nn.Conv2d(in, out, k)` | 2D convolution | Images -- captures local spatial patterns |\n",
        "| `nn.ReLU()` | Non-linearity max(0,x) | Default activation after most layers |\n",
        "| `nn.Sigmoid()` | Squash to [0,1] | Binary output probability |\n",
        "| `nn.Dropout(p)` | Random zero activations | Prevent overfitting |\n",
        "| `nn.BatchNorm1d/2d` | Normalize activations | Stabilize training, allow higher LR |\n",
        "| `nn.Embedding(V, D)` | Learnable lookup table | NLP word embeddings |\n",
        "| `nn.LSTM(in, hidden)` | Recurrent layer | Sequences -- text, time series |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40961b8",
      "metadata": {
        "id": "f40961b8"
      },
      "outputs": [],
      "source": [
        "# nn.Sequential -- clean syntax for simple stacks\n",
        "mlp = nn.Sequential(\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "print(mlp)\n",
        "\n",
        "total = sum(p.numel() for p in mlp.parameters())\n",
        "print(f\"\\nTotal parameters: {total:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab198c39",
      "metadata": {
        "id": "ab198c39"
      },
      "source": [
        "### Loss Functions -- Why the Choice Matters\n",
        "\n",
        "The loss function defines what \"wrong\" means. Wrong choice = model optimizes the wrong objective.\n",
        "\n",
        "| Loss | Use When | Why |\n",
        "|------|----------|-----|\n",
        "| `MSELoss` | Regression | Penalizes large errors heavily (squares them) |\n",
        "| `L1Loss` | Regression with outliers | Treats all errors equally, more robust |\n",
        "| `CrossEntropyLoss` | Multi-class classification | Most common for classification. Combines LogSoftmax + NLL. |\n",
        "| `BCEWithLogitsLoss` | Binary classification | Numerically stable. Use instead of Sigmoid + BCE. |\n",
        "\n",
        "**Common mistake:** Using MSELoss for classification treats class labels as ordered numbers -- meaningless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6b96e2",
      "metadata": {
        "id": "3e6b96e2"
      },
      "outputs": [],
      "source": [
        "# MSELoss\n",
        "mse = nn.MSELoss()\n",
        "pred   = torch.tensor([2.5, 3.0, 4.0])\n",
        "target = torch.tensor([2.0, 3.0, 5.0])\n",
        "print(f\"MSE Loss: {mse(pred, target).item():.4f}\")\n",
        "\n",
        "# CrossEntropyLoss -- input is raw LOGITS (not softmax!), target is class indices\n",
        "ce = nn.CrossEntropyLoss()\n",
        "logits = torch.tensor([[2.0, 1.0, 0.5],\n",
        "                        [0.5, 2.5, 0.3]])\n",
        "labels = torch.tensor([0, 1])\n",
        "print(f\"CrossEntropy Loss: {ce(logits, labels).item():.4f}\")\n",
        "\n",
        "# BCEWithLogitsLoss\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "b_logits = torch.tensor([1.5, -0.5, 2.0])\n",
        "b_labels = torch.tensor([1.0,  0.0, 1.0])\n",
        "print(f\"BCE Loss: {bce(b_logits, b_labels).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c031e14",
      "metadata": {
        "id": "7c031e14"
      },
      "source": [
        "### Optimizers -- Why Not Just Gradient Descent?\n",
        "\n",
        "**Vanilla SGD problem:** Same LR for all parameters. Gets stuck in saddle points. Noisy mini-batch updates.\n",
        "\n",
        "**Adam** tracks a running average of gradient AND squared gradient per parameter -- giving each its own adaptive learning rate. Parameters with consistently large gradients get a smaller effective LR; rarely-updated ones get larger.\n",
        "\n",
        "**AdamW** fixes Adam's weight decay bug -- decouples L2 regularization from the adaptive gradient update. Use AdamW for most modern work.\n",
        "\n",
        "**When SGD wins:** For image CNNs, SGD + momentum + LR schedule often achieves better final accuracy than Adam. Adam trains faster but converges to sharper minima that generalize worse.\n",
        "\n",
        "**Good defaults:** Adam/AdamW: `3e-4`, SGD: `0.01-0.1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b15ae4",
      "metadata": {
        "id": "b3b15ae4"
      },
      "outputs": [],
      "source": [
        "model = LinearModel()\n",
        "sgd_opt   = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "adam_opt  = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "adamw_opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "\n",
        "# Compare convergence on same problem\n",
        "results = {}\n",
        "for name, opt_cls, lr in [(\"SGD\",   torch.optim.SGD,   0.1),\n",
        "                            (\"Adam\",  torch.optim.Adam,  3e-4),\n",
        "                            (\"AdamW\", torch.optim.AdamW, 3e-4)]:\n",
        "    m = LinearModel()\n",
        "    opt = opt_cls(m.parameters(), lr=lr)\n",
        "    lf = nn.MSELoss()\n",
        "    ep_losses = []\n",
        "    for _ in range(50):\n",
        "        p = m(X); l = lf(p, y)\n",
        "        opt.zero_grad(); l.backward(); opt.step()\n",
        "        ep_losses.append(l.item())\n",
        "    results[name] = ep_losses\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "for name, ls in results.items():\n",
        "    plt.plot(ls, label=name)\n",
        "plt.yscale('log')\n",
        "plt.title('SGD vs Adam vs AdamW -- Convergence')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "plt.legend(); plt.grid(True); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ca18119",
      "metadata": {
        "id": "9ca18119"
      },
      "source": [
        "---\n",
        "## 7. The Standard Training Loop\n",
        "\n",
        "### Why `model.train()` vs `model.eval()`?\n",
        "\n",
        "Some layers behave **differently** during training vs inference:\n",
        "\n",
        "- **Dropout:** Train = randomly zeros neurons. Eval = uses all neurons (deterministic).\n",
        "- **BatchNorm:** Train = normalizes using current batch stats. Eval = uses accumulated running stats.\n",
        "\n",
        "Forgetting `model.eval()` is a **silent bug** -- model still runs but gives inconsistent, wrong predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c2bbc9",
      "metadata": {
        "id": "13c2bbc9"
      },
      "outputs": [],
      "source": [
        "# Demonstrate train vs eval mode\n",
        "dropout_demo = nn.Sequential(nn.Linear(10, 10), nn.Dropout(0.5))\n",
        "x_test = torch.ones(1, 10)\n",
        "\n",
        "dropout_demo.train()\n",
        "out1 = dropout_demo(x_test)\n",
        "out2 = dropout_demo(x_test)\n",
        "print(\"Train mode (random, non-deterministic):\")\n",
        "print(f\"  Same results? {torch.allclose(out1, out2)}\")\n",
        "\n",
        "dropout_demo.eval()\n",
        "out3 = dropout_demo(x_test)\n",
        "out4 = dropout_demo(x_test)\n",
        "print(\"Eval mode (deterministic):\")\n",
        "print(f\"  Same results? {torch.allclose(out3, out4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008f4bde",
      "metadata": {
        "id": "008f4bde"
      },
      "outputs": [],
      "source": [
        "# THE STANDARD TRAINING LOOP -- copy this as your base template\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, loss_fn, num_epochs=10):\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ---- TRAINING ----\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # move to device if using GPU: X_batch = X_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss   = loss_fn(y_pred, y_batch)\n",
        "\n",
        "            optimizer.zero_grad()   # zero FIRST\n",
        "            loss.backward()         # compute gradients\n",
        "            optimizer.step()        # update parameters\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # ---- VALIDATION ----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():       # no graph = memory savings\n",
        "            for X_val, y_val in val_loader:\n",
        "                val_pred = model(X_val)\n",
        "                val_loss += loss_fn(val_pred, y_val).item()\n",
        "\n",
        "        avg_train = train_loss / len(train_loader)\n",
        "        avg_val   = val_loss   / len(val_loader)\n",
        "        train_losses.append(avg_train)\n",
        "        val_losses.append(avg_val)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
        "                  f\"Train: {avg_train:.4f} | Val: {avg_val:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "print(\"Training loop template defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba84580",
      "metadata": {
        "id": "7ba84580"
      },
      "source": [
        "---\n",
        "## 8. Data -- Dataset and DataLoader\n",
        "\n",
        "### Why Not Just Index Your Data Manually?\n",
        "\n",
        "You could do `X[i:i+32]`. But DataLoader provides:\n",
        "\n",
        "- **Shuffling** -- breaks spurious patterns the model could overfit to\n",
        "- **Batching** -- GPUs process many samples in parallel\n",
        "- **Parallel loading** (`num_workers`) -- load data in background while GPU computes\n",
        "- **Pin memory** -- speeds up CPU-to-GPU data transfer\n",
        "\n",
        "### Batch size guidance:\n",
        "- Too small (1-8): Noisy gradients, poor GPU utilization  \n",
        "- Too large (>512): May generalize worse, requires more memory  \n",
        "- Sweet spot: **32 to 256** for most tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539904c7",
      "metadata": {
        "id": "539904c7"
      },
      "outputs": [],
      "source": [
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        # Required: total number of samples\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Required: return one sample by index\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "X_all = torch.randn(200, 1)\n",
        "y_all = 3 * X_all + 2 + 0.1 * torch.randn(200, 1)\n",
        "\n",
        "train_dataset = RegressionDataset(X_all[:160], y_all[:160])\n",
        "val_dataset   = RegressionDataset(X_all[160:], y_all[160:])\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} samples\")\n",
        "print(f\"Val:   {len(val_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cafc139",
      "metadata": {
        "id": "2cafc139"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,       # shuffle every epoch -- important for training\n",
        "    num_workers=0,      # 0 = main process; set to 4 for large image datasets\n",
        "    # pin_memory=True   # uncomment when training on GPU\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False       # no need to shuffle validation\n",
        ")\n",
        "\n",
        "X_batch, y_batch = next(iter(train_loader))\n",
        "print(f\"Batch X: {X_batch.shape}\")   # [32, 1]\n",
        "print(f\"Batch y: {y_batch.shape}\")   # [32, 1]\n",
        "print(f\"Number of training batches: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858a69f4",
      "metadata": {
        "id": "858a69f4"
      },
      "outputs": [],
      "source": [
        "# Full training run combining everything\n",
        "torch.manual_seed(42)\n",
        "model_full = LinearModel()\n",
        "optimizer = torch.optim.AdamW(model_full.parameters(), lr=3e-2)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_losses, val_losses = train_model(\n",
        "    model_full, train_loader, val_loader,\n",
        "    optimizer, loss_fn, num_epochs=20\n",
        ")\n",
        "\n",
        "print(f\"\\nW: {model_full.linear.weight.item():.4f}  (true: 3.0)\")\n",
        "print(f\"b: {model_full.linear.bias.item():.4f}  (true: 2.0)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0dba695",
      "metadata": {
        "id": "a0dba695"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(train_losses, label='Train', marker='o', markersize=4)\n",
        "plt.plot(val_losses,   label='Val',   marker='s', markersize=4)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "plt.title('Train vs Val Loss')\n",
        "plt.legend(); plt.grid(True); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a985537f",
      "metadata": {
        "id": "a985537f"
      },
      "source": [
        "---\n",
        "## 9. GPU Usage\n",
        "\n",
        "### Why Manual Device Management?\n",
        "\n",
        "PyTorch is explicit about where each tensor lives -- precise control for multi-GPU, limited VRAM, or mixed pipelines.\n",
        "\n",
        "**The Golden Rule:** Model and data must be on the **same device**.\n",
        "\n",
        "Most common error:\n",
        "```\n",
        "RuntimeError: Expected all tensors to be on the same device\n",
        "```\n",
        "Fix: `.to(device)` on both model and data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a56f5f",
      "metadata": {
        "id": "c4a56f5f"
      },
      "outputs": [],
      "source": [
        "# Device selection -- works on Colab GPU, Apple Silicon, and CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")   # Apple Silicon\n",
        "    print(\"Apple Silicon MPS\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU only\")\n",
        "\n",
        "print(f\"Using: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e54bc3",
      "metadata": {
        "id": "82e54bc3"
      },
      "outputs": [],
      "source": [
        "# Move model to device\n",
        "model_gpu = LinearModel().to(device)\n",
        "print(f\"Model device: {next(model_gpu.parameters()).device}\")\n",
        "\n",
        "# In training loop, move each batch to device\n",
        "for X_batch, y_batch in train_loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    y_pred = model_gpu(X_batch)   # both on same device\n",
        "    break\n",
        "\n",
        "print(f\"Batch device: {X_batch.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853e4156",
      "metadata": {
        "id": "853e4156"
      },
      "source": [
        "---\n",
        "## 10. Saving & Loading Models\n",
        "\n",
        "### Why `state_dict()` Instead of the Whole Model?\n",
        "\n",
        "`torch.save(model)` saves the entire Python object -- tied to your class definition and file structure. Rename anything, upgrade PyTorch, and it may break.\n",
        "\n",
        "`state_dict()` saves just the **parameter tensors as a plain dictionary** -- portable, stable, version-independent. This is the professional standard.\n",
        "\n",
        "**Pattern:** Save state_dict. Load into freshly-constructed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af96dc4",
      "metadata": {
        "id": "4af96dc4"
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "torch.save(model_full.state_dict(), \"linear_model.pth\")\n",
        "print(\"Saved.\")\n",
        "\n",
        "sd = model_full.state_dict()\n",
        "print(f\"Keys: {list(sd.keys())}\")\n",
        "for k, v in sd.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1defb2",
      "metadata": {
        "id": "5b1defb2"
      },
      "outputs": [],
      "source": [
        "# Load\n",
        "loaded_model = LinearModel()\n",
        "loaded_model.load_state_dict(\n",
        "    torch.load(\"linear_model.pth\", map_location=device)\n",
        ")\n",
        "loaded_model.eval()   # ALWAYS set eval for inference!\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_x = torch.tensor([[1.0]])\n",
        "    pred = loaded_model(test_x)\n",
        "    print(f\"Prediction for x=1.0: {pred.item():.4f}  (expected ~5.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0740cb62",
      "metadata": {
        "id": "0740cb62"
      },
      "source": [
        "---\n",
        "## 11. The Batch Dimension -- Why Data Is Always `[B, ...]`\n",
        "\n",
        "Nearly every tensor in PyTorch has a **batch dimension first**:\n",
        "\n",
        "| Data type | Shape |\n",
        "|-----------|-------|\n",
        "| Images | `[batch, channels, height, width]` |\n",
        "| Text | `[batch, sequence_length]` |\n",
        "| Tabular | `[batch, features]` |\n",
        "\n",
        "**Why?** GPUs parallelize over the batch dimension. Processing 32 samples simultaneously costs essentially the same compute as processing 1. Model weights are shared across all samples.\n",
        "\n",
        "**Single-sample inference:** You still need to add a batch dimension with `unsqueeze(0)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7233f386",
      "metadata": {
        "id": "7233f386"
      },
      "outputs": [],
      "source": [
        "# Inference on a single sample\n",
        "single = X_all[0]                          # shape [1] -- just features\n",
        "print(f\"Single sample: {single.shape}\")\n",
        "\n",
        "single_batched = single.unsqueeze(0)       # shape [1, 1] -- with batch dim\n",
        "print(f\"With batch:    {single_batched.shape}\")\n",
        "\n",
        "loaded_model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = loaded_model(single_batched)\n",
        "    print(f\"Prediction: {pred.item():.4f}\")\n",
        "    print(f\"Actual:     {y_all[0].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63ddb139",
      "metadata": {
        "id": "63ddb139"
      },
      "source": [
        "---\n",
        "## 12. The Full Mental Model\n",
        "\n",
        "```\n",
        "Raw Data\n",
        "   |\n",
        "   v  Dataset + transforms\n",
        "Tensors in Batches\n",
        "   |\n",
        "   v  DataLoader (shuffled, parallel)\n",
        "Forward Pass\n",
        "   |\n",
        "   v  nn.Module layers\n",
        "Predictions\n",
        "   |\n",
        "   v  Loss Function\n",
        "Scalar Loss\n",
        "   |\n",
        "   v  loss.backward()\n",
        "Gradients\n",
        "   |\n",
        "   v  optimizer.step()\n",
        "Updated Parameters\n",
        "   |\n",
        "   v  repeat for N epochs\n",
        "Trained Model\n",
        "```\n",
        "\n",
        "### Every Component Has a Clear Role\n",
        "\n",
        "| Component | Role | Without it... |\n",
        "|-----------|------|---------------|\n",
        "| **Tensor** | Holds data + parameters | No computation |\n",
        "| **Autograd** | Computes gradients automatically | Manual calculus for every model |\n",
        "| **nn.Module** | Organizes parameters + layers | Messy manual tracking |\n",
        "| **Loss Function** | Defines what \"wrong\" means | Model optimizes the wrong thing |\n",
        "| **Optimizer** | Updates parameters intelligently | Slow / unstable convergence |\n",
        "| **DataLoader** | Feeds data efficiently | No parallelism, no shuffling |\n",
        "| **train()/eval()** | Controls layer behavior | Silent wrong predictions |\n",
        "| **no_grad()** | Saves memory at inference | OOM errors, slow inference |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b0b1956",
      "metadata": {
        "id": "9b0b1956"
      },
      "source": [
        "---\n",
        "## Quick Reference Cheatsheet\n",
        "\n",
        "```python\n",
        "# TENSOR CREATION\n",
        "torch.zeros(3, 4)\n",
        "torch.randn(3, 4)\n",
        "torch.tensor([1.0, 2.0])\n",
        "torch.arange(0, 10, 2)\n",
        "\n",
        "# SHAPES\n",
        "x.shape, x.ndim, x.dtype\n",
        "x.reshape(3, -1)          # -1 = infer\n",
        "x.unsqueeze(0)            # add batch dim\n",
        "x.squeeze()               # remove size-1 dims\n",
        "x.to(device)              # move to GPU/CPU\n",
        "\n",
        "# AUTOGRAD\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "loss.backward()           # compute gradients\n",
        "optimizer.zero_grad()     # ALWAYS before backward()\n",
        "with torch.no_grad(): ... # inference / validation\n",
        "\n",
        "# TRAINING LOOP\n",
        "# model.train()\n",
        "# pred = model(X); loss = fn(pred, y)\n",
        "# optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "\n",
        "# VALIDATION\n",
        "# model.eval()\n",
        "# with torch.no_grad(): pred = model(X)\n",
        "\n",
        "# DEVICE\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "tensor.to(device)\n",
        "\n",
        "# SAVE / LOAD\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()  # after loading!\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}